\chapter{Информационные характеристики источников{ } \\
 дискретных сообщений}
\ttl
\textbf{Задача 1.39}. Ансамбли событий ${X}$ и ${Y}$ объединены, причём вероятности совместных событий равны: $p(x_1, y_1)=0,007$; $p(x_1, y_2)=0,151$; $p(x_1, y_3)=0,217$; $p(x_1, y_4)=0,14$; $p(x_2, y_1)=0,023$; $p(x_2, y_2)=0,011$; $p(x_2, y_3)=0,18$; $p(x_2, y_4)\hm=0,271$.Найти: энтропии ансамблей ${X}$ и ${Y}$ соответственно ${H(X)}$, ${H(Y)}$; энтропию объединенного ансамбля ${H(X, Y)}$; 
условные энтропии ансамблей ${H(X/Y)}$, ${H(Y/X)}$.
\begin{center}\textbf{Решение}\end{center}

Для определения энтропий ${H(X)}$, ${H(Y)}$, необходимо знать безусловные вероятности $p(x_{1})$, $p(x_{2})$, $p(y_{1})$, $p(y_{2})$, $p(y_{3})$, $p(y_{4})$.
Зная закон распределения двумерной случайной величины, можно найти закон распределения её составляющих:
$$
p(x_i)=\sum_{i=1}^n p(x_i, y_j).\eqno(1)
$$

$p(x_{1})=p(x_1, y_1) + p(x_1, y_2) + p(x_1, y_3) + p(x_1, y_4) = 0,007 + 0,151 + 0,217\hm + 0,14 = 0,515$;

$p(x_{2}) = 1 - p(x_{1}) = 1-0,515 = 0,485$;

$p(y_{1}) = p(x_1, y_1) + p(x_2, y_1) = 0,007 + 0,023 = 0,03$;

$p(y_{2}) = p(x_1, y_2) + p(x_2, y_2) = 0,151 + 0,11 = 0,162$;

$p(y_{3}) = p(x_1, y_3) + p(x_2, y_3) = 0,217 + 0,18 = 0,397$;

$p(y_{4}) = p(x_1, y_4) + p(x_2, y_4) = 0,14 + 0,271 = 0,411$.

Энтропия ансамбля рассчитывается по формуле:
$$
H(x)=-\sum_{i=1}^n p(x_i)\log_2 p(x_i).\eqno(2)
$$

$ H(X) = -0,515\log_2 0,515 - 0,485\log_2 0,485 = 0,999 $ (бит);

$ H(Y) = -0,03\log_2 0,03 - 0,162\log_2 0,162 -0,397\log_2 0,397 -0,411\times \linebreak \times\log_2 0,411\hm = 0,152+0,423+0,529+0,527 = 1,631 $(бит).

Энтропия объединения нескольких статистически независимых источников информации равна сумме энтропии исходных источников :
$$
H(X,Y) = -\sum_{i=1}^N \sum_{j=1}^M p(x_i, y_j) \log_2 p(x_i, y_j).
$$

$H(X,Y) = - p(x_1, y_1)\log_2 p(x_1, y_1) - p(x_1, y_2) \log_2(x_1, y_2) - p(x_1, y_3)\times \linebreak \times\log_2 p(x_1, y_3)\hm - p(x_1, y_4)\log_2 p(x_1, y_4) - p(x_2, y_1)\log_2 p(x_2, y_1) - p(x_2,y_2)\times \linebreak \times \log_2p(x_2,y_2) - p(x_2, y_3)\log_2p(x_2, y_3)\hm - p(x_2, y_4)\log_2 p(x_2, y_4) = - 0,007\times \linebreak \times \log_2 0,007 - 0,151\log_2 0,151 - 0,217\log_2 0,217 - 0,14\log_2 0,14 - 0,023\times \linebreak \times\log_2 0,023 - 0,011\log_2 0,011 - 0,18\log_2 0,18\hm - 0,271\log_2 0,271 = 2,491$ (бит).

Выведем формулу для условной энтропии:

$\displaystyle H(Y/X) = -\sum_{i=1}^N \sum_{j=1}^K p(y_i, x_j) \log_2 \frac{p(y_i, x_j)}{p(x_j)} = -\sum_{i=1}^N \sum_{j=1}^K  p(y_i, x_j)\times \linebreak \times \log_2 p(y_i, x_j) + \sum_{i=1}^N p(y_i, x_j) \sum_{j=1}^K \log_2 p(x_j)  = H(X,Y)-H(X).$

Из этого соотношения найдём условные энтропии ансамблей:

$H(Y/X) = H(X,Y)-H(X) = 2,491-0,999 = 1,492$ (бит);

$H(X/Y) = H(X,Y)-H(Y) = 2,491-1,631 = 0,86$ (бит).

\textbf{Ответ}: $H(X)$ = 0,999 бит, $H(Y)$ = 1,631 бит, $H(X, Y)$ = 2,491 бит,\linebreak $H(Y/X)$\hm = 1,492 бит, $H(X/Y)~0,86 $ бит.

\label{1.69}
\textbf{Задача 1.69}. Принимаемый сигнал может иметь амплитуду $A_1$ (событие $X_1$) или $A_2$ (событие $X_2$), а также сдвиг фазы $\varphi_1$ (событие $Y_1$) или $\varphi_2$ (событие $Y_2$). Вероятности совместных событий имеют следующие значения: $p(x_1, y_1) = 0,4$; $p(x_1, y_2) = 0,12$; $p(x_2, y_1) = 0,08$; $p(x_2, y_2) = 0,4$. Вычислить количество информации, получаемой о фазовом сдвиге сигнала, если станет известной его амплитуда.
\begin{center}\textbf{Решение}\end{center}

В задаче требуется найти взаимную информацию. Взаимная информа\linebreakция~--- статистическая функция двух случайных величин, описывающая коли-\linebreakчество  информации, содержащееся в одной случайной величине относительно \linebreak другой. 

Взаимная информация определяется через энтропию и условную энтропию двух случайных величин как
$$
I(X,Y) = H(Y) - H(Y/X).\eqno(3)
$$

Для определения энтропии $H(X)$, $H(Y)$, необходимо знать безусловные вероятности $p(x_1)$, $p(x_2)$, $p(y_1)$, $p(y_1)$. По формуле (1):

$p(y_1) = p(x_1, y_1) + p(x_2, y_1) = 0,4 + 0,08 = 0,48$;

$p(y_2) = 1 - p(y_1) = 1-0,48 = 0,52$;

$p(x_1) = p(x_1, y_1) + p(x_1, y_2) = 0,4 + 0,12 = 0,52$;

$p(x_2) = 1- p(x_1) = 1-0,52 = 0,48$.

По формуле (2) энтропия ансамбля $H(Y)$:

$H(Y) = -0,48\log_2 0,48 - 0,52\log_2 0,52 = 0,998$ (бит);

Условная энтропия события $Y$ относительно $X$ определяется следующим образом:
$$
H(Y/X) = -\sum_{i=1}^N \sum_{j=1}^M p(y_i, x_j)\log_2 \frac{p(y_i, x_j)}{p(x_j)}.
$$

$H(Y/X) = -p(x_1, y_1)\log_2 \frac{p(x_1, y_1)}{p(x_1)}\hm -p(x_1, y_2)\log_2  \frac{p(x_1, y_2)}{p(x_1)}\hm-p(x_2, y_1)\hm\log_2  \frac{p(x_2, y_1)}{p(x_2)}\hm-p(x_2, y_2)\log_2  \frac{p(x_2, y_2)}{p(x_2)} = -0,4\log_2 0,77 -0,12\log_2 0,23 -0,08\log_2 0,17 -0,4\log_2 0,83\hm = 0,15-0,25-0,2-0,12 = 0,72 $(бит).

По формуле (3) находим взаимную информацию:

$I(X,Y) = 0,998 - 0,72 = 0.278$ (бит).

\textbf{Ответ}: $I(X, Y) = 0,278$ бит.

\label{1.87}
\textbf{Задача 1.87}. Дискретный источник выбирает сообщения из ансамбля 
$U\hm = \left(\begin{array}{cccc}
u_1 & u_2 & u_3 & u_4\\
0,14 & 0,22 & 0,29 & 0,35
 \end{array}\right)$.
Длительности сообщений соответственно равны $t_{u_1}$ = 0,43 с, $t_{u_2}$ = 0,26 с, $t_{u_3}$ = 0,09 с, $t_{u_4}$ = 0,093~с. Определить производительность источника.
\begin{center}\textbf{Решение}\end{center}
Под производительностью источника сообщений подразумевают количество информации, вырабатываемое источником в единицу времени:
$$
H'(U) = \frac{1}{T} H(U).\eqno(4)
$$

Так как длительность выдачи знаков источником в каждом из состояний различна, то средняя длительность выдачи источником одного знака найдётся как математическое ожидание от распределения времени:
$$
T= \sum_{i=1}^n t_i p(t_i).
$$

$T=0,14\cdot0,43 + 0,22\cdot0,26 + 0,29\cdot0,09 + 0,35\cdot0,93 = 0,469$ (c).

По формуле (2) энтропия источника:

$H(U) = -0,14\log_2 0,14 -0,22\log_2 0,22 -0,29\log_2 0,29 -0,35\log_2 0,35\hm = 0,397+0.481+0,518+0,530= 1,926$ (бит).

С помощью формулы (4) найдём производительность источника:

$H'(U) = \frac{1,962}{0,469} = 4,18 $ (бит/с).

\textbf{Ответ}: $H'(U)  = 4,18$ бит/с.
